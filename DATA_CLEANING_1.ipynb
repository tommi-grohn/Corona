{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting and cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "\n",
    "### 1.1. Data\n",
    "\n",
    "\n",
    "- Our World in Data COVID-19 dataset\n",
    "    - 'Our World in Data'-organisation has collected a lot of interesting data about the COVID-19 from different sources including ECDC, United Nations, World Bank and Global Burden of Disease. \n",
    "    - This research paper's most essential data collected by them is the data on infected and deaths in different countries.\n",
    "\n",
    "\n",
    "- Google's traffic data \n",
    "    - Google provides anonymized insights from products such as Google Maps for researchers to help them to make critical analysis to combat COVID-19. \n",
    "    - Google has divided their traffic data into six traffic components: \n",
    "        1. retail \\& recreation\n",
    "            - places like restaurants, cafes, shopping centers, theme parks, museums, libraries and movie theaters\n",
    "        2. grocery \\& pharmacy\n",
    "            - places like grocery markets, food warehouses, farmers markets, specialty food shops, drug stores and pharmacies\n",
    "        3. parks\n",
    "            - places like national parks, public beaches, marinas, dog parks, plazas and public gardens\n",
    "        4. transit stations\n",
    "            - places like public transport hubs such as subway, bus and train stations\n",
    "        5. workplaces \n",
    "            - places of work\n",
    "        6. residential \n",
    "            -  places of residence\n",
    "     \n",
    "  - These components do not tell anything how much time people spend in each section on average but they still give a lot of information how people's traffic behavior changed during the pandemic\n",
    " \n",
    " - The traffic data's baseline is counted as a median value of multiple days. Day-to-day changes should not be emphasized too much because they are effected on many different factors, f.e. the weather and public events.  \n",
    "\n",
    "\n",
    "- Regional infected data from different sources\n",
    "    - For now there is only one example: Spain. It's data is from datos.gob.es (Open data initiative of the Government of Spain)\n",
    "\n",
    "\n",
    "### 1.2. Moral\n",
    "\n",
    "\n",
    "- Moral of this notebook is to clean data here and add it to csv-files. Then in the main notebook csv-files can be read with less data cleaning which simplifies the main notebook.\n",
    "\n",
    "\n",
    "\n",
    "### 1.3. Structure\n",
    "\n",
    "\n",
    "1. Introduction\n",
    "2. Download different datasets\n",
    "3. Create df_days_by_countries.csv \n",
    "4. Create df_countries.csv\n",
    "5. Create df_days_by_regions.csv\n",
    "6. Create df_regions.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Libraries used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Parameters which need to be defined manually\n",
    "\n",
    "- All the parameters, which need to be defined manually, are here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "\n",
    "# There is COVID-19-data from February until now\n",
    "observations_start_date = datetime.datetime(2020, 2, 1, 0, 0)\n",
    "observations_end_date = datetime.datetime(2020, 8, 18, 0, 0)\n",
    "\n",
    "# However, the data analysis of this notebook concentrates on summer months, i.e. on so called tail\n",
    "tail_start_date = datetime.datetime(2020, 6, 1, 0, 0)\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# European countries ordered by population\n",
    "european_countries = [\n",
    "    'Germany', 'United Kingdom', 'France', 'Italy', 'Spain', \n",
    "    'Ukraine', 'Poland', 'Romania','Netherlands', 'Belgium', \n",
    "    'Greece', 'Sweden', 'Portugal', 'Hungary', 'Belarus', \n",
    "    'Austria', 'Switzerland','Bulgaria', 'Serbia', 'Denmark', \n",
    "    'Finland', 'Norway', 'Ireland', 'Croatia', 'Bosnia and Herzegovina',\n",
    "    'Lithuania', 'Moldova', 'Slovenia','Estonia' ]\n",
    "\n",
    "# There are some countries which have no regional data available\n",
    "countries_with_no_regional_data = ['Ukraine', 'Belarus', 'Serbia', 'Bosnia and Herzegovina', 'Moldova']\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# Window size for convolution\n",
    "w = 7\n",
    "w2 = 28\n",
    "\n",
    "# Death limit\n",
    "d = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29 European countries analysed in this notebook.\n",
      "The length of the whole interval: 199\n",
      "The length of the tail interval: 78\n"
     ]
    }
   ],
   "source": [
    "# Follows directly from manual definitions \n",
    "num_countries = len(european_countries)\n",
    "whole_interval_len = (observations_end_date - observations_start_date).days\n",
    "tail_interval_len = (observations_end_date - tail_start_date).days\n",
    "date_list = [observations_start_date + datetime.timedelta(days=x) for x in range(whole_interval_len)]\n",
    "\n",
    "print(\"There are \" + str(num_countries) + \" European countries analysed in this notebook.\")\n",
    "print(\"The length of the whole interval: \" + str(whole_interval_len))\n",
    "print(\"The length of the tail interval: \" + str(tail_interval_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Dataframes which are turned into csv-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe df_countries\n",
    "\n",
    "- The length of this dataframe is 'num_countries'. Indeed, for each country there is one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataframe sorted by countries\n",
    "\n",
    "dtypes_countries = np.dtype([\n",
    "          ('country', str),\n",
    "          ('group', float),\n",
    "          ('population', int),\n",
    "          ('population_in_millions', int),\n",
    "          ('significance_level', float),\n",
    "          ('control_point', str),\n",
    "          ('escalation_point_deaths', str),\n",
    "          ('deaths_escalated_rapidly', str),\n",
    "          ('escalation_point_infections', str),\n",
    "          ('infections_escalated_rapidly', str),\n",
    "          ])\n",
    "\n",
    "data_countries = np.empty(0, dtype=dtypes_countries)\n",
    "df_countries = pd.DataFrame(data_countries)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe df_days_by_countries\n",
    "\n",
    "- All the days to which there exists traffic and infected data to each country\n",
    "\n",
    "\n",
    "- The length of the dataframe equals 'num_countries' * 'whole_interval_len'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A countrywise sorted dataframe s.t. for each day on the time interval of each country there is a row \n",
    "\n",
    "dtypes_days_by_countries = np.dtype([\n",
    "          ('country', str),  # country name\n",
    "          ('date', np.datetime64), # current date\n",
    "          ('new_infections', int), # new infections on that date\n",
    "          ('new_infections_smooth', int), # smoothened new infections on that date\n",
    "          ('new_deaths', int), # new deaths on that date\n",
    "          ('new_deaths_smooth', int), # smoothened new deaths on that date\n",
    "          ('total_deaths_per_million', float), # how many deaths per million has occured until that date\n",
    "          ('traffic_retail', float), # retail and recreation traffic on that date\n",
    "          ('traffic_supermarket', float), # supermarket and pharmacy traffic on that date\n",
    "          ('traffic_parks', float),  # park traffic on that date\n",
    "          ('traffic_transit_stations', float), # transit station traffic on that date\n",
    "          ('traffic_workplaces', float), # workplace traffic on that date\n",
    "          ('traffic_residential', float), # residential traffic on that date\n",
    "          ])\n",
    "data_days_by_countries = np.empty(0, dtype=dtypes_days_by_countries)\n",
    "df_days_by_countries = pd.DataFrame(data_days_by_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe df_regions\n",
    "\n",
    "\n",
    "- Each country consists of smaller regions.\n",
    "\n",
    "\n",
    "- For each region there is one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A countrywise sorted dataframe s.t. for each day on the time interval of each country there is a row \n",
    "\n",
    "dtypes_regions = np.dtype([\n",
    "          ('country', str),  # country name\n",
    "          ('region', str),   # region name\n",
    "          ('group', int),    # the group of the region. Remark: This should be str!\n",
    "          ])\n",
    "data_regions = np.empty(0, dtype=dtypes_regions)\n",
    "df_regions = pd.DataFrame(data_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe df_days_by_regions\n",
    "\n",
    "\n",
    "- Similar to the dataframe 'df_days_by_countries' but instead of a country, each row equals a specific day of a region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A countrywise sorted dataframe s.t. for each day on the time interval of each country there is a row \n",
    "\n",
    "dtypes_days_by_regions = np.dtype([\n",
    "          ('country', str),  # country name\n",
    "          ('region', str),  # country name\n",
    "          ('date', np.datetime64), # current date\n",
    "          ('new_infections', int), # new infections. These information is found out only for some regions!\n",
    "          ('traffic_retail', float), # retail and recreation traffic on that date\n",
    "          ('traffic_supermarket', float), # supermarket and pharmacy traffic on that date\n",
    "          ('traffic_parks', float),  # park traffic on that date\n",
    "          ('traffic_transit_stations', float), # transit station traffic on that date\n",
    "          ('traffic_workplaces', float), # workplace traffic on that date\n",
    "          ('traffic_residential', float), # residential traffic on that date\n",
    "          ('traffic_retail_smooth', float), # retail and recreation traffic on that date\n",
    "          ('traffic_supermarket_smooth', float), # supermarket and pharmacy traffic on that date\n",
    "          ('traffic_parks_smooth', float),  # park traffic on that date\n",
    "          ('traffic_transit_stations_smooth', float), # transit station traffic on that date\n",
    "          ('traffic_workplaces_smooth', float), # workplace traffic on that date\n",
    "          ('traffic_residential_smooth', float), # residential traffic on that datep\n",
    "          ])\n",
    "data_days_by_regions = np.empty(0, dtype=dtypes_days_by_regions)\n",
    "df_days_by_regions = pd.DataFrame(data_days_by_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Download 'Our World in Data COVID-19'- dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the 'Our World in Data COVID-19'- dataset and create a dataframe\n",
    "\n",
    "def load_covid_data():\n",
    "    #url = 'owid-covid-data.csv'\n",
    "    url = 'https://covid.ourworldindata.org/data/owid-covid-data.csv'\n",
    "    \n",
    "    data = pd.read_csv(url)\n",
    "    \n",
    "    # CONSIDER IF THIS NEEDS TO BE FIXED OR NOT!!\n",
    "    # Fix a clear false datapoint\n",
    "    #data.loc[(data.location == \"Spain\") & (data.date == '2020-06-19'), 'new_deaths'] = 0\n",
    "    \n",
    "    return data\n",
    "\n",
    "df_covid_data = load_covid_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Download Google's traffic data by countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommi/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_region_code</th>\n",
       "      <th>country_region</th>\n",
       "      <th>date</th>\n",
       "      <th>retail_and_recreation_percent_change_from_baseline</th>\n",
       "      <th>grocery_and_pharmacy_percent_change_from_baseline</th>\n",
       "      <th>parks_percent_change_from_baseline</th>\n",
       "      <th>transit_stations_percent_change_from_baseline</th>\n",
       "      <th>workplaces_percent_change_from_baseline</th>\n",
       "      <th>residential_percent_change_from_baseline</th>\n",
       "      <th>supermarket_and_pharmacy_percent_change_from_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>2020-02-18</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905678</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-08-17</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905679</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-08-18</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905680</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-08-19</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905681</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905682</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25380 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        country_region_code        country_region       date  \\\n",
       "0                        AE  United Arab Emirates 2020-02-15   \n",
       "1                        AE  United Arab Emirates 2020-02-16   \n",
       "2                        AE  United Arab Emirates 2020-02-17   \n",
       "3                        AE  United Arab Emirates 2020-02-18   \n",
       "4                        AE  United Arab Emirates 2020-02-19   \n",
       "...                     ...                   ...        ...   \n",
       "1905678                  ZW              Zimbabwe 2020-08-17   \n",
       "1905679                  ZW              Zimbabwe 2020-08-18   \n",
       "1905680                  ZW              Zimbabwe 2020-08-19   \n",
       "1905681                  ZW              Zimbabwe 2020-08-20   \n",
       "1905682                  ZW              Zimbabwe 2020-08-21   \n",
       "\n",
       "         retail_and_recreation_percent_change_from_baseline  \\\n",
       "0                                                      0.0    \n",
       "1                                                      1.0    \n",
       "2                                                     -1.0    \n",
       "3                                                     -2.0    \n",
       "4                                                     -2.0    \n",
       "...                                                    ...    \n",
       "1905678                                              -16.0    \n",
       "1905679                                              -13.0    \n",
       "1905680                                               -9.0    \n",
       "1905681                                               -7.0    \n",
       "1905682                                               -9.0    \n",
       "\n",
       "         grocery_and_pharmacy_percent_change_from_baseline  \\\n",
       "0                                                      4.0   \n",
       "1                                                      4.0   \n",
       "2                                                      1.0   \n",
       "3                                                      1.0   \n",
       "4                                                      0.0   \n",
       "...                                                    ...   \n",
       "1905678                                              -11.0   \n",
       "1905679                                               -6.0   \n",
       "1905680                                                1.0   \n",
       "1905681                                               -1.0   \n",
       "1905682                                                1.0   \n",
       "\n",
       "         parks_percent_change_from_baseline  \\\n",
       "0                                       5.0   \n",
       "1                                       4.0   \n",
       "2                                       5.0   \n",
       "3                                       5.0   \n",
       "4                                       4.0   \n",
       "...                                     ...   \n",
       "1905678                                 5.0   \n",
       "1905679                                 2.0   \n",
       "1905680                                 3.0   \n",
       "1905681                                 3.0   \n",
       "1905682                                 4.0   \n",
       "\n",
       "         transit_stations_percent_change_from_baseline  \\\n",
       "0                                                  0.0   \n",
       "1                                                  1.0   \n",
       "2                                                  1.0   \n",
       "3                                                  0.0   \n",
       "4                                                 -1.0   \n",
       "...                                                ...   \n",
       "1905678                                          -29.0   \n",
       "1905679                                          -30.0   \n",
       "1905680                                          -26.0   \n",
       "1905681                                          -27.0   \n",
       "1905682                                          -26.0   \n",
       "\n",
       "         workplaces_percent_change_from_baseline  \\\n",
       "0                                            2.0   \n",
       "1                                            2.0   \n",
       "2                                            2.0   \n",
       "3                                            2.0   \n",
       "4                                            2.0   \n",
       "...                                          ...   \n",
       "1905678                                    -19.0   \n",
       "1905679                                    -19.0   \n",
       "1905680                                    -19.0   \n",
       "1905681                                    -19.0   \n",
       "1905682                                    -16.0   \n",
       "\n",
       "         residential_percent_change_from_baseline  \\\n",
       "0                                             1.0   \n",
       "1                                             1.0   \n",
       "2                                             1.0   \n",
       "3                                             1.0   \n",
       "4                                             1.0   \n",
       "...                                           ...   \n",
       "1905678                                      25.0   \n",
       "1905679                                      25.0   \n",
       "1905680                                      23.0   \n",
       "1905681                                      24.0   \n",
       "1905682                                      24.0   \n",
       "\n",
       "         supermarket_and_pharmacy_percent_change_from_baseline  \n",
       "0                                                      4.0      \n",
       "1                                                      4.0      \n",
       "2                                                      1.0      \n",
       "3                                                      1.0      \n",
       "4                                                      0.0      \n",
       "...                                                    ...      \n",
       "1905678                                              -11.0      \n",
       "1905679                                               -6.0      \n",
       "1905680                                                1.0      \n",
       "1905681                                               -1.0      \n",
       "1905682                                                1.0      \n",
       "\n",
       "[25380 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A function that cleans traffic data and adds it to a temporary dataframe \n",
    "def load_mobility_raw_onlynan_by_countries():\n",
    "    #url = 'Global_Mobility_Report.csv'\n",
    "    url = 'https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv'\n",
    "    \n",
    "    data = pd.read_csv(url)\n",
    "    data = data[data['sub_region_1'].isna()]\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "    \n",
    "    data['retail_and_recreation_percent_change_from_baseline']=data.groupby('country_region_code')['retail_and_recreation_percent_change_from_baseline'].fillna(method='ffill')\n",
    "    data['retail_and_recreation_percent_change_from_baseline']=data.groupby('country_region_code')['retail_and_recreation_percent_change_from_baseline'].fillna(method='bfill')\n",
    "    data['supermarket_and_pharmacy_percent_change_from_baseline']=data.groupby('country_region_code')['grocery_and_pharmacy_percent_change_from_baseline'].fillna(method='ffill')\n",
    "    data['supermarket_and_pharmacy_percent_change_from_baseline']=data.groupby('country_region_code')['grocery_and_pharmacy_percent_change_from_baseline'].fillna(method='bfill')\n",
    "    data['parks_percent_change_from_baseline']=data.groupby('country_region_code')['parks_percent_change_from_baseline'].fillna(method='ffill')\n",
    "    data['parks_percent_change_from_baseline']=data.groupby('country_region_code')['parks_percent_change_from_baseline'].fillna(method='bfill')\n",
    "    data['transit_stations_percent_change_from_baseline']=data.groupby('country_region_code')['transit_stations_percent_change_from_baseline'].fillna(method='ffill')\n",
    "    data['transit_stations_percent_change_from_baseline']=data.groupby('country_region_code')['transit_stations_percent_change_from_baseline'].fillna(method='bfill')\n",
    "    data['workplaces_percent_change_from_baseline']=data.groupby('country_region_code')['workplaces_percent_change_from_baseline'].fillna(method='ffill')\n",
    "    data['workplaces_percent_change_from_baseline']=data.groupby('country_region_code')['workplaces_percent_change_from_baseline'].fillna(method='bfill')\n",
    "    data['residential_percent_change_from_baseline']=data.groupby('country_region_code')['residential_percent_change_from_baseline'].fillna(method='ffill')\n",
    "    data['residential_percent_change_from_baseline']=data.groupby('country_region_code')['residential_percent_change_from_baseline'].fillna(method='bfill')\n",
    "        \n",
    "    data=data.dropna(subset=['country_region'])\n",
    "\n",
    "    # Drop all non NaN values from metro_area\n",
    "    data = data[pd.isnull(data['metro_area'])]\n",
    "    \n",
    "    data=data.drop(columns=['sub_region_2','sub_region_1', 'metro_area', 'iso_3166_2_code', 'census_fips_code'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "df_traffic_temp_by_countries = load_mobility_raw_onlynan_by_countries()\n",
    "df_traffic_temp_by_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Download Google's traffic data by regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommi/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_region</th>\n",
       "      <th>sub_region_1</th>\n",
       "      <th>date</th>\n",
       "      <th>retail_and_recreation_percent_change_from_baseline</th>\n",
       "      <th>grocery_and_pharmacy_percent_change_from_baseline</th>\n",
       "      <th>parks_percent_change_from_baseline</th>\n",
       "      <th>transit_stations_percent_change_from_baseline</th>\n",
       "      <th>workplaces_percent_change_from_baseline</th>\n",
       "      <th>residential_percent_change_from_baseline</th>\n",
       "      <th>supermarket_and_pharmacy_percent_change_from_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81415</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Burgenland</td>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81416</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Burgenland</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81417</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Burgenland</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81418</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Burgenland</td>\n",
       "      <td>2020-02-18</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81419</th>\n",
       "      <td>Austria</td>\n",
       "      <td>Burgenland</td>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250732</th>\n",
       "      <td>Slovenia</td>\n",
       "      <td>Žalec</td>\n",
       "      <td>2020-08-17</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250733</th>\n",
       "      <td>Slovenia</td>\n",
       "      <td>Žalec</td>\n",
       "      <td>2020-08-18</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250734</th>\n",
       "      <td>Slovenia</td>\n",
       "      <td>Žalec</td>\n",
       "      <td>2020-08-19</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250735</th>\n",
       "      <td>Slovenia</td>\n",
       "      <td>Žalec</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250736</th>\n",
       "      <td>Slovenia</td>\n",
       "      <td>Žalec</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108264 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        country_region sub_region_1       date  \\\n",
       "81415          Austria   Burgenland 2020-02-15   \n",
       "81416          Austria   Burgenland 2020-02-16   \n",
       "81417          Austria   Burgenland 2020-02-17   \n",
       "81418          Austria   Burgenland 2020-02-18   \n",
       "81419          Austria   Burgenland 2020-02-19   \n",
       "...                ...          ...        ...   \n",
       "1250732       Slovenia        Žalec 2020-08-17   \n",
       "1250733       Slovenia        Žalec 2020-08-18   \n",
       "1250734       Slovenia        Žalec 2020-08-19   \n",
       "1250735       Slovenia        Žalec 2020-08-20   \n",
       "1250736       Slovenia        Žalec 2020-08-21   \n",
       "\n",
       "         retail_and_recreation_percent_change_from_baseline  \\\n",
       "81415                                                 -3.0    \n",
       "81416                                                 15.0    \n",
       "81417                                                  9.0    \n",
       "81418                                                  6.0    \n",
       "81419                                                  3.0    \n",
       "...                                                    ...    \n",
       "1250732                                               -5.0    \n",
       "1250733                                               -5.0    \n",
       "1250734                                               -5.0    \n",
       "1250735                                               -5.0    \n",
       "1250736                                               -5.0    \n",
       "\n",
       "         grocery_and_pharmacy_percent_change_from_baseline  \\\n",
       "81415                                                 -3.0   \n",
       "81416                                                  NaN   \n",
       "81417                                                  6.0   \n",
       "81418                                                  5.0   \n",
       "81419                                                  4.0   \n",
       "...                                                    ...   \n",
       "1250732                                               -5.0   \n",
       "1250733                                                NaN   \n",
       "1250734                                                NaN   \n",
       "1250735                                                NaN   \n",
       "1250736                                              -15.0   \n",
       "\n",
       "         parks_percent_change_from_baseline  \\\n",
       "81415                                  57.0   \n",
       "81416                                  47.0   \n",
       "81417                                  24.0   \n",
       "81418                                  28.0   \n",
       "81419                                  12.0   \n",
       "...                                     ...   \n",
       "1250732                                 NaN   \n",
       "1250733                                 NaN   \n",
       "1250734                                 NaN   \n",
       "1250735                                 NaN   \n",
       "1250736                                 NaN   \n",
       "\n",
       "         transit_stations_percent_change_from_baseline  \\\n",
       "81415                                             21.0   \n",
       "81416                                             11.0   \n",
       "81417                                             12.0   \n",
       "81418                                              8.0   \n",
       "81419                                              9.0   \n",
       "...                                                ...   \n",
       "1250732                                            NaN   \n",
       "1250733                                            NaN   \n",
       "1250734                                            NaN   \n",
       "1250735                                            NaN   \n",
       "1250736                                            NaN   \n",
       "\n",
       "         workplaces_percent_change_from_baseline  \\\n",
       "81415                                       -7.0   \n",
       "81416                                       -2.0   \n",
       "81417                                        3.0   \n",
       "81418                                        2.0   \n",
       "81419                                        2.0   \n",
       "...                                          ...   \n",
       "1250732                                    -35.0   \n",
       "1250733                                    -32.0   \n",
       "1250734                                    -31.0   \n",
       "1250735                                    -32.0   \n",
       "1250736                                    -31.0   \n",
       "\n",
       "         residential_percent_change_from_baseline  \\\n",
       "81415                                        -2.0   \n",
       "81416                                        -1.0   \n",
       "81417                                        -2.0   \n",
       "81418                                         0.0   \n",
       "81419                                        -1.0   \n",
       "...                                           ...   \n",
       "1250732                                       0.0   \n",
       "1250733                                       0.0   \n",
       "1250734                                       0.0   \n",
       "1250735                                       0.0   \n",
       "1250736                                       0.0   \n",
       "\n",
       "         supermarket_and_pharmacy_percent_change_from_baseline  \n",
       "81415                                                 -3.0      \n",
       "81416                                                  6.0      \n",
       "81417                                                  6.0      \n",
       "81418                                                  5.0      \n",
       "81419                                                  4.0      \n",
       "...                                                    ...      \n",
       "1250732                                               -5.0      \n",
       "1250733                                              -15.0      \n",
       "1250734                                              -15.0      \n",
       "1250735                                              -15.0      \n",
       "1250736                                              -15.0      \n",
       "\n",
       "[108264 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A function that cleans traffic data and adds it to a temporary dataframe \n",
    "def load_mobility_raw_onlynan_by_regions():\n",
    "    #url = 'Global_Mobility_Report.csv'\n",
    "    url = 'https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv'\n",
    "    \n",
    "    data = pd.read_csv(url)\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "    \n",
    "    data['retail_and_recreation_percent_change_from_baseline']=data.groupby('sub_region_1')['retail_and_recreation_percent_change_from_baseline'].fillna(method='ffill')\n",
    "    data['retail_and_recreation_percent_change_from_baseline']=data.groupby('sub_region_1')['retail_and_recreation_percent_change_from_baseline'].fillna(method='bfill')\n",
    "    data['supermarket_and_pharmacy_percent_change_from_baseline']=data.groupby('sub_region_1')['grocery_and_pharmacy_percent_change_from_baseline'].fillna(method='ffill')\n",
    "    data['supermarket_and_pharmacy_percent_change_from_baseline']=data.groupby('sub_region_1')['grocery_and_pharmacy_percent_change_from_baseline'].fillna(method='bfill')\n",
    "    data['parks_percent_change_from_baseline']=data.groupby('sub_region_1')['parks_percent_change_from_baseline'].fillna(method='ffill')\n",
    "    data['parks_percent_change_from_baseline']=data.groupby('sub_region_1')['parks_percent_change_from_baseline'].fillna(method='bfill')\n",
    "    data['transit_stations_percent_change_from_baseline']=data.groupby('sub_region_1')['transit_stations_percent_change_from_baseline'].fillna(method='ffill')\n",
    "    data['transit_stations_percent_change_from_baseline']=data.groupby('sub_region_1')['transit_stations_percent_change_from_baseline'].fillna(method='bfill')\n",
    "    data['workplaces_percent_change_from_baseline']=data.groupby('sub_region_1')['workplaces_percent_change_from_baseline'].fillna(method='ffill')\n",
    "    data['workplaces_percent_change_from_baseline']=data.groupby('sub_region_1')['workplaces_percent_change_from_baseline'].fillna(method='bfill')\n",
    "    data['residential_percent_change_from_baseline']=data.groupby('sub_region_1')['residential_percent_change_from_baseline'].fillna(method='ffill')\n",
    "    data['residential_percent_change_from_baseline']=data.groupby('sub_region_1')['residential_percent_change_from_baseline'].fillna(method='bfill')\n",
    "        \n",
    "    data=data.dropna(subset=['sub_region_1'])\n",
    "    \n",
    "    # Drop all non NaN values from metro_area\n",
    "    data = data[pd.isnull(data['metro_area'])]\n",
    "    \n",
    "    # Drop all non NaN values from sub_region_2\n",
    "    data = data[pd.isnull(data['sub_region_2'])]\n",
    "    \n",
    "    # Drop all unnecessary columns\n",
    "    data=data.drop(columns=['country_region_code', 'sub_region_2', 'metro_area', 'iso_3166_2_code', 'census_fips_code'])\n",
    "    \n",
    "    # Filter European countries\n",
    "    data = data[data['country_region'].isin(european_countries)]\n",
    "    \n",
    "    return data\n",
    "\n",
    "df_traffic_temp_by_regions = load_mobility_raw_onlynan_by_regions()\n",
    "df_traffic_temp_by_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Download regional infected data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Netherlands\n",
    "\n",
    "- Reading the csv-file of Netherlands does not work unless there is manually added a column name 'Error_value' to the spot which would be in H1 in an excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://data.rivm.nl/covid-19/\n",
    "data_netherlands = pd.read_csv(\"Data/Regional_infected_data/COVID-19_aantallen_gemeente_cumulatief.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datos.gob.es/en/catalogo/e05070101-evolucion-de-enfermedad-por-el-coronavirus-covid-19\n",
    "data_spain = pd.read_csv(\"Data/Regional_infected_data/datos_ccaas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create df_days_by_countries.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Update columns 'new_infected', 'new_deaths' and 'total_deaths_per_million'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell prints a warning if observations_end_date was chosen to be too large\n",
    "# EVENTUALLY THIS CELL IS TAKEN AWAY!\n",
    "\n",
    "if df_covid_data[(df_covid_data['date'] < observations_end_date.strftime('%Y-%m-%d'))& (df_covid_data['location'] == \"Germany\")]['date'].tolist()[-1] != (observations_end_date - datetime.timedelta(1)).strftime('%Y-%m-%d'):\n",
    "    raise ValueError(\"observations_end_date was chosen to be too large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 62 missing death values 5771\n"
     ]
    }
   ],
   "source": [
    "# Update these column-values to the dataframe df_days\n",
    "columns = ['country', 'date', 'new_infected', 'new_deaths', 'total_deaths_per_million']\n",
    "\n",
    "# Counts how many missing death values there were in the data\n",
    "num_nan_death_values = 0\n",
    "\n",
    "date_list = [observations_start_date + datetime.timedelta(days=x) for x in range(whole_interval_len)]\n",
    "\n",
    "\n",
    "# Loop over each European country\n",
    "for i in range(num_countries):\n",
    "    \n",
    "    # Define the country's name and what its time interval's start and end date is\n",
    "    current_country = european_countries[i]\n",
    "    \n",
    "    # Create a temporary dataframe for the current country with the correct time interval\n",
    "    df_covid_data_current = df_covid_data[(df_covid_data['date'] >= observations_start_date.strftime('%Y-%m-%d'))&\n",
    "                  (df_covid_data['date'] < observations_end_date.strftime('%Y-%m-%d'))&\n",
    "                  (df_covid_data['location'] == current_country)]\n",
    "        \n",
    "    # Get the information new infections, deaths and total deaths per million in separate lists\n",
    "    new_infections = df_covid_data_current['new_cases'].tolist()\n",
    "    new_deaths = df_covid_data_current['new_deaths'].tolist()\n",
    "    total_deaths_per_million = df_covid_data_current['total_deaths_per_million'].tolist()\n",
    "    \n",
    "    # It is possible the counting of infections or deaths does not start early enough. \n",
    "    # Then add 0s to the beginning of the list\n",
    "    if len(new_infections) < whole_interval_len:\n",
    "        n = whole_interval_len - len(new_infections)\n",
    "        new_infections = [0] * n + new_infections\n",
    "        new_deaths = [0] * n + new_deaths\n",
    "        total_deaths_per_million = [0] * n + total_deaths_per_million\n",
    "    \n",
    "    # There are a few NaN and one negative value in columns that need to be cleaned separately!\n",
    "    # For these values there is set a value after the missing one. If that exists neither, a value 0 is chosen.\n",
    "    for j in range(whole_interval_len -1):\n",
    "        if np.isnan(new_infections[j]) or new_infections[j] < 0:\n",
    "            if np.isnan(new_infections[j+1]):\n",
    "                new_infections[j] = 0\n",
    "            else:\n",
    "                new_infections[j] = new_infections[j+1]\n",
    "                \n",
    "        if np.isnan(new_deaths[j]) or new_deaths[j] < 0:\n",
    "            if np.isnan(new_deaths[j+1]):\n",
    "                new_deaths[j] = 0\n",
    "            else:\n",
    "                new_deaths[j] = new_deaths[j+1]\n",
    "            num_nan_death_values += 1\n",
    "            \n",
    "        if np.isnan(total_deaths_per_million[j]) or total_deaths_per_million[j] < 0:\n",
    "            if np.isnan(total_deaths_per_million[j+1]):\n",
    "                total_deaths_per_million[j] = 0\n",
    "            else:\n",
    "                total_deaths_per_million[j] = total_deaths_per_million[j+1]\n",
    "            \n",
    "    # Now we can update all current_interval_len rows of the current country to the dataframe df_days\n",
    "    new_data = {'country':[current_country] * whole_interval_len, 'date': date_list, \n",
    "                'new_infections': new_infections, 'new_deaths': new_deaths,\n",
    "                'total_deaths_per_million': total_deaths_per_million } \n",
    "    \n",
    "    df_extention = pd.DataFrame(new_data)\n",
    "    df_days_by_countries = pd.concat([df_days_by_countries, df_extention])\n",
    "\n",
    "print(\"There were \" + str(num_nan_death_values) + \" missing death values \" + str(df_days_by_countries.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Add smoothened infected and death data to the dataframe\n",
    "\n",
    "- Let's denote the total number of new deaths in a country $c$ on a day $t$ with $N_{t,c}^{(D)}$.\n",
    "\n",
    "\n",
    "- Then let's define smoothened death data $\\bar{D}_{t, c} = \\frac{1}{w} \\sum_{\\tau = \\text{max}(0, t - w + 1)}^{t} N_{\\tau,c}^{(D)}$\n",
    "    - A logical choice for $w$ is f.e. 7 or 14 because then in smoothened deaths there is taken normal weekly changes in consideration.\n",
    "        - In general, people move differently on Tuesdays compared to Sundays.\n",
    "        \n",
    "\n",
    "- Similarly confirmed infections $\\bar{C}_{t, c}$ is defined with exactly same logic from the total number of new infections $N_{t,c}^{(C)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Container lists\n",
    "new_infections_smooth = []\n",
    "new_deaths_smooth = []\n",
    "\n",
    "# Loop over each European country\n",
    "for i in range(num_countries):\n",
    "    \n",
    "    # Define the current country, a temporary dataframe of the country and x-axis (dates)\n",
    "    current_country = european_countries[i]\n",
    "    df_current = df_days_by_countries[(df_days_by_countries['country'] == current_country)]\n",
    "        \n",
    "    # Roll a window over the infected data to get it smoothened\n",
    "    df_current['new_infections_smooth']=df_current.new_infections.rolling(window = w, win_type=None).mean()\n",
    "    df_current['new_infections_smooth']=df_current.new_infections_smooth.fillna(method='ffill')\n",
    "    df_current['new_infections_smooth']=df_current.new_infections_smooth.fillna(method='bfill').apply(np.int64)\n",
    "    \n",
    "    # Roll a window over the death data to get it smoothened\n",
    "    df_current['new_deaths_smooth']=df_current.new_deaths.rolling(window = w, win_type=None).mean()\n",
    "    df_current['new_deaths_smooth']=df_current.new_deaths_smooth.fillna(method='ffill')\n",
    "    df_current['new_deaths_smooth']=df_current.new_deaths_smooth.fillna(method='bfill').apply(np.int64)\n",
    "    \n",
    "    # Add the current smoothened infected and death data to the container lists\n",
    "    new_infections_smooth = new_infections_smooth + df_current['new_infections_smooth'].tolist()\n",
    "    new_deaths_smooth = new_deaths_smooth + df_current['new_deaths_smooth'].tolist()\n",
    "\n",
    "# Update the dataframe columns at once\n",
    "df_days_by_countries['new_infections_smooth'] = new_infections_smooth\n",
    "df_days_by_countries['new_deaths_smooth'] = new_deaths_smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Add Google's traffic data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell prints a warning if observations_end_date was chosen to be too large\n",
    "# EVENTUALLY THIS CELL IS TAKEN AWAY!\n",
    "\n",
    "if df_traffic_temp_by_countries[(df_traffic_temp_by_countries['date'] < observations_end_date.strftime('%Y-%m-%d'))&(df_traffic_temp_by_countries['country_region'] == \"Germany\")]['date'].tolist()[-1] != (observations_end_date - datetime.timedelta(1)):\n",
    "    raise ValueError(\"observations_end_date was chosen to be too large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update traffic data values to the dataframe df_days\n",
    "\n",
    "traffic_retail = []\n",
    "traffic_supermarket = []\n",
    "traffic_parks = []\n",
    "traffic_transit_stations = []\n",
    "traffic_workplaces = []\n",
    "traffic_residential = []\n",
    "\n",
    "\n",
    "# Loop over each European country\n",
    "for current_country in european_countries:\n",
    "    \n",
    "    # Create a temporary dataframe for the current country with the correct time interval\n",
    "    df0_current = df_traffic_temp_by_countries[(df_traffic_temp_by_countries['date'] >= observations_start_date.strftime('%Y-%m-%d'))&\n",
    "                  (df_traffic_temp_by_countries['date'] < observations_end_date.strftime('%Y-%m-%d'))&\n",
    "                  (df_traffic_temp_by_countries['country_region'] == current_country)]\n",
    "    \n",
    "    # Get the information about different traffic components in separate lists\n",
    "    new_total_retail = df0_current['retail_and_recreation_percent_change_from_baseline'].tolist()\n",
    "    new_total_supermarket = df0_current['supermarket_and_pharmacy_percent_change_from_baseline'].tolist()\n",
    "    new_total_parks = df0_current['parks_percent_change_from_baseline'].tolist()\n",
    "    new_total_transit_stations = df0_current['transit_stations_percent_change_from_baseline'].tolist()\n",
    "    new_total_workplaces = df0_current['workplaces_percent_change_from_baseline'].tolist()\n",
    "    new_total_residential = df0_current['residential_percent_change_from_baseline'].tolist()\n",
    "\n",
    "    # In few countries (f.e. Italy) the traffic data does not start early enough \n",
    "    # Then there is added 0 values (baseline traffic values) at the beginning of the list \n",
    "    if len(new_total_retail) < whole_interval_len:\n",
    "        new_total_retail = [0] * (whole_interval_len-len(new_total_retail)) + new_total_retail\n",
    "        new_total_supermarket = [0] * (whole_interval_len-len(new_total_supermarket)) + new_total_supermarket\n",
    "        new_total_parks = [0] * (whole_interval_len-len(new_total_parks)) + new_total_parks\n",
    "        new_total_transit_stations = [0] * (whole_interval_len-len(new_total_transit_stations)) + new_total_transit_stations\n",
    "        new_total_workplaces = [0] * (whole_interval_len-len(new_total_workplaces)) + new_total_workplaces\n",
    "        new_total_residential = [0] * (whole_interval_len-len(new_total_residential)) + new_total_residential\n",
    "    \n",
    "    # Update the traffic container-vectors\n",
    "    if len(new_total_retail) !=  whole_interval_len:\n",
    "        print(current_country)\n",
    "    \n",
    "    traffic_retail = traffic_retail + new_total_retail\n",
    "    traffic_supermarket = traffic_supermarket + new_total_supermarket\n",
    "    traffic_parks = traffic_parks + new_total_parks\n",
    "    traffic_transit_stations = traffic_transit_stations + new_total_transit_stations\n",
    "    traffic_workplaces = traffic_workplaces + new_total_workplaces\n",
    "    traffic_residential = traffic_residential + new_total_residential\n",
    "    \n",
    "    \n",
    "# Update all the traffic values for all countries and days at once\n",
    "df_days_by_countries['traffic_retail'] = traffic_retail\n",
    "df_days_by_countries['traffic_supermarket'] = traffic_supermarket\n",
    "df_days_by_countries['traffic_parks'] = traffic_parks\n",
    "df_days_by_countries['traffic_transit_stations'] = traffic_transit_stations\n",
    "df_days_by_countries['traffic_workplaces'] = traffic_workplaces\n",
    "df_days_by_countries['traffic_residential'] = traffic_residential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Create a csv-file out of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>date</th>\n",
       "      <th>new_infections</th>\n",
       "      <th>new_infections_smooth</th>\n",
       "      <th>new_deaths</th>\n",
       "      <th>new_deaths_smooth</th>\n",
       "      <th>total_deaths_per_million</th>\n",
       "      <th>traffic_retail</th>\n",
       "      <th>traffic_supermarket</th>\n",
       "      <th>traffic_parks</th>\n",
       "      <th>traffic_transit_stations</th>\n",
       "      <th>traffic_workplaces</th>\n",
       "      <th>traffic_residential</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Germany</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>2020-02-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Germany</td>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Estonia</td>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.492</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Estonia</td>\n",
       "      <td>2020-08-14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.492</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Estonia</td>\n",
       "      <td>2020-08-15</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.492</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Estonia</td>\n",
       "      <td>2020-08-16</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.492</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Estonia</td>\n",
       "      <td>2020-08-17</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.492</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5771 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     country       date  new_infections  new_infections_smooth  new_deaths  \\\n",
       "0    Germany 2020-02-01             2.0                      1         0.0   \n",
       "1    Germany 2020-02-02             1.0                      1         0.0   \n",
       "2    Germany 2020-02-03             1.0                      1         0.0   \n",
       "3    Germany 2020-02-04             2.0                      1         0.0   \n",
       "4    Germany 2020-02-05             0.0                      1         0.0   \n",
       "..       ...        ...             ...                    ...         ...   \n",
       "194  Estonia 2020-08-13             7.0                      8         0.0   \n",
       "195  Estonia 2020-08-14             0.0                      7         0.0   \n",
       "196  Estonia 2020-08-15             3.0                      6         0.0   \n",
       "197  Estonia 2020-08-16             7.0                      5         0.0   \n",
       "198  Estonia 2020-08-17             6.0                      5         0.0   \n",
       "\n",
       "     new_deaths_smooth  total_deaths_per_million  traffic_retail  \\\n",
       "0                    0                     0.000             0.0   \n",
       "1                    0                     0.000             0.0   \n",
       "2                    0                     0.000             0.0   \n",
       "3                    0                     0.000             0.0   \n",
       "4                    0                     0.000             0.0   \n",
       "..                 ...                       ...             ...   \n",
       "194                  0                    47.492             7.0   \n",
       "195                  0                    47.492             3.0   \n",
       "196                  0                    47.492            -1.0   \n",
       "197                  0                    47.492            11.0   \n",
       "198                  0                    47.492            13.0   \n",
       "\n",
       "     traffic_supermarket  traffic_parks  traffic_transit_stations  \\\n",
       "0                    0.0            0.0                       0.0   \n",
       "1                    0.0            0.0                       0.0   \n",
       "2                    0.0            0.0                       0.0   \n",
       "3                    0.0            0.0                       0.0   \n",
       "4                    0.0            0.0                       0.0   \n",
       "..                   ...            ...                       ...   \n",
       "194                  8.0           92.0                      -9.0   \n",
       "195                  8.0          145.0                      -5.0   \n",
       "196                  7.0          194.0                       6.0   \n",
       "197                 12.0          231.0                      13.0   \n",
       "198                 12.0          164.0                      -2.0   \n",
       "\n",
       "     traffic_workplaces  traffic_residential  \n",
       "0                   0.0                  0.0  \n",
       "1                   0.0                  0.0  \n",
       "2                   0.0                  0.0  \n",
       "3                   0.0                  0.0  \n",
       "4                   0.0                  0.0  \n",
       "..                  ...                  ...  \n",
       "194               -36.0                  4.0  \n",
       "195               -35.0                  1.0  \n",
       "196                 1.0                 -8.0  \n",
       "197                 5.0                -11.0  \n",
       "198               -35.0                  1.0  \n",
       "\n",
       "[5771 rows x 13 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_days_by_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = os.path.join('Fully_implemented_dataframes', 'df_days_by_countries.csv')\n",
    "df_days_by_countries.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create df_countries.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Update columns 'population' and 'population_in_millions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container lists\n",
    "populations = []\n",
    "populations_in_millions = []\n",
    "\n",
    "for current_country in european_countries:\n",
    "    \n",
    "    # Add current population to the lists 'populations' and 'populations_in_millions'\n",
    "    populations.append(int(round(df_covid_data.loc[(df_covid_data['location'] == current_country), 'population'].values[0])))\n",
    "    populations_in_millions.append(int(round(df_covid_data.loc[(df_covid_data['location'] == current_country), 'population'].values[0] / 1000000)))\n",
    "    \n",
    "    \n",
    "# Update the dataframe columns at once\n",
    "df_countries['country'] = european_countries\n",
    "df_countries['population'] = populations    \n",
    "df_countries['population_in_millions'] = populations_in_millions    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create a csv-file out of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = os.path.join('Temporary_dataframes', 'df_countries.csv')\n",
    "df_countries.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create df_days_by_regions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Add Google's traffic data \n",
    "\n",
    "- Ignore regions with too many NaN-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was dropped 107 regions out of 587 because these regions had too many NaN-values\n"
     ]
    }
   ],
   "source": [
    "# Update traffic data values to the dataframe df_days\n",
    "\n",
    "traffic_retail = []\n",
    "traffic_supermarket = []\n",
    "traffic_parks = []\n",
    "traffic_transit_stations = []\n",
    "traffic_workplaces = []\n",
    "traffic_residential = []\n",
    "\n",
    "\n",
    "countries_repeated = []\n",
    "regions_repeated = []\n",
    "dates_repeated = []\n",
    "\n",
    "\n",
    "date_list = [observations_start_date + datetime.timedelta(days=x) for x in range(whole_interval_len)]\n",
    "\n",
    "# Create a list of all regions overall in the world\n",
    "european_regions = list(dict.fromkeys(df_traffic_temp_by_regions['sub_region_1'].tolist()))\n",
    "\n",
    "counter_for_dropped_regions = 0\n",
    "\n",
    "# Loop over each European country\n",
    "for current_region in european_regions:\n",
    "    \n",
    "    # Create a temporary dataframe for the current country with the correct time interval\n",
    "    df0_current = df_traffic_temp_by_regions[(df_traffic_temp_by_regions['date'] >= observations_start_date.strftime('%Y-%m-%d'))&\n",
    "                  (df_traffic_temp_by_regions['date'] < observations_end_date.strftime('%Y-%m-%d'))&\n",
    "                  (df_traffic_temp_by_regions['sub_region_1'] == current_region)]\n",
    "    \n",
    "    # The current country of the region\n",
    "    current_country = df0_current['country_region'].iloc[0]\n",
    "\n",
    "    # Get the information about different traffic components in separate lists\n",
    "    new_total_retail = df0_current['retail_and_recreation_percent_change_from_baseline'].tolist()\n",
    "    new_total_supermarket = df0_current['supermarket_and_pharmacy_percent_change_from_baseline'].tolist()\n",
    "    new_total_parks = df0_current['parks_percent_change_from_baseline'].tolist()\n",
    "    new_total_transit_stations = df0_current['transit_stations_percent_change_from_baseline'].tolist()\n",
    "    new_total_workplaces = df0_current['workplaces_percent_change_from_baseline'].tolist()\n",
    "    new_total_residential = df0_current['residential_percent_change_from_baseline'].tolist()\n",
    "\n",
    "\n",
    "    # If a region has too many NaN-values, the region will be not taken into account\n",
    "    region_has_too_many_nans = False\n",
    "    for current_traffic_data in [new_total_retail, new_total_supermarket, new_total_parks, new_total_transit_stations, new_total_workplaces, new_total_residential]:\n",
    "        nan_counter = 0\n",
    "        for current_element in current_traffic_data:\n",
    "            if np.isnan(current_element):\n",
    "                nan_counter += 1\n",
    "        if nan_counter > 10:\n",
    "            region_has_too_many_nans = True        \n",
    "            \n",
    "    if region_has_too_many_nans: \n",
    "        counter_for_dropped_regions += 1\n",
    "    \n",
    "    # A region will be added to the dataframe only if region_has_too_many_nans == False\n",
    "    if region_has_too_many_nans == False:\n",
    "    \n",
    "        # In few countries (f.e. Italy) the traffic data does not start early enough \n",
    "        # Then there is added 0 values (baseline traffic values) at the beginning of the list \n",
    "        if len(new_total_retail) < whole_interval_len:\n",
    "            new_total_retail = [0] * (whole_interval_len-len(new_total_retail)) + new_total_retail\n",
    "            new_total_supermarket = [0] * (whole_interval_len-len(new_total_supermarket)) + new_total_supermarket\n",
    "            new_total_parks = [0] * (whole_interval_len-len(new_total_parks)) + new_total_parks\n",
    "            new_total_transit_stations = [0] * (whole_interval_len-len(new_total_transit_stations)) + new_total_transit_stations\n",
    "            new_total_workplaces = [0] * (whole_interval_len-len(new_total_workplaces)) + new_total_workplaces\n",
    "            new_total_residential = [0] * (whole_interval_len-len(new_total_residential)) + new_total_residential\n",
    "\n",
    "\n",
    "        # Update the traffic container-vectors\n",
    "        traffic_retail = traffic_retail + new_total_retail\n",
    "        traffic_supermarket = traffic_supermarket + new_total_supermarket\n",
    "        traffic_parks = traffic_parks + new_total_parks\n",
    "        traffic_transit_stations = traffic_transit_stations + new_total_transit_stations\n",
    "        traffic_workplaces = traffic_workplaces + new_total_workplaces\n",
    "        traffic_residential = traffic_residential + new_total_residential\n",
    "\n",
    "\n",
    "        countries_repeated = countries_repeated + [current_country] * len(date_list)\n",
    "        regions_repeated = regions_repeated + [current_region] * len(date_list)\n",
    "        dates_repeated = dates_repeated + date_list\n",
    "    \n",
    "    \n",
    "# Update all the traffic values for all countries and days at once\n",
    "df_days_by_regions['country'] = countries_repeated\n",
    "df_days_by_regions['region'] = regions_repeated\n",
    "df_days_by_regions['date'] = dates_repeated\n",
    "\n",
    "df_days_by_regions['traffic_retail'] = traffic_retail\n",
    "df_days_by_regions['traffic_supermarket'] = traffic_supermarket\n",
    "df_days_by_regions['traffic_parks'] = traffic_parks\n",
    "df_days_by_regions['traffic_transit_stations'] = traffic_transit_stations\n",
    "df_days_by_regions['traffic_workplaces'] = traffic_workplaces\n",
    "df_days_by_regions['traffic_residential'] = traffic_residential\n",
    "\n",
    "\n",
    "print(\"There was dropped \" + str(counter_for_dropped_regions) + \" regions out of \" + str(len(european_regions) ) + \" because these regions had too many NaN-values\")\n",
    "\n",
    "#df_days_by_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Add countries which don't have regional data to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_country in countries_with_no_regional_data:\n",
    "    df0_current = df_days_by_countries[(df_days_by_countries['country'] == current_country)]\n",
    "\n",
    "    # Get the information about different traffic components in separate lists\n",
    "    traffic_retail = df0_current['traffic_retail'].tolist()\n",
    "    traffic_supermarket = df0_current['traffic_supermarket'].tolist()\n",
    "    traffic_parks = df0_current['traffic_parks'].tolist()\n",
    "    traffic_transit_stations = df0_current['traffic_transit_stations'].tolist()\n",
    "    traffic_workplaces = df0_current['traffic_workplaces'].tolist()\n",
    "    traffic_residential = df0_current['traffic_residential'].tolist()\n",
    "    \n",
    "    # Now we can update all current_interval_len rows of the current country to the dataframe df_days\n",
    "    new_data = {'country':[current_country] * whole_interval_len,\n",
    "                'region':[current_country] * whole_interval_len,\n",
    "                'date': date_list, 'traffic_retail' : traffic_retail, \n",
    "                'traffic_supermarket': traffic_supermarket, 'traffic_parks': traffic_parks,\n",
    "                'traffic_transit_stations': traffic_transit_stations, \n",
    "                'traffic_workplaces': traffic_workplaces, 'traffic_residential': traffic_residential} \n",
    "    \n",
    "    df_extention = pd.DataFrame(new_data)\n",
    "    df_days_by_regions = pd.concat([df_days_by_regions, df_extention])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Update smoothened traffic components in the dataframe\n",
    "\n",
    "\n",
    "- Same way as smoothened deaths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change weekends to NaN-values what comes to workplace traffic\n",
    "\n",
    "- People don't work during weekend in general. Therefore, the workplace data for the weekend should not have too much emphasis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These days run over the all Saturdays and Sundays during the whole time interval\n",
    "running_saturday = datetime.datetime(2020, 2, 1, 0, 0)\n",
    "running_sunday = datetime.datetime(2020, 2, 2, 0, 0)\n",
    "\n",
    "while running_saturday <= datetime.datetime(2020, 8, 31, 0, 0):\n",
    "    \n",
    "    # Update the running Saturday and Sunday to NaN values \n",
    "    df_days_by_regions.loc[(df_days_by_regions['date'] == running_saturday), 'traffic_workplaces'] = np.nan\n",
    "    df_days_by_regions.loc[(df_days_by_regions['date'] == running_sunday), 'traffic_workplaces'] = np.nan\n",
    "    \n",
    "    # Update the running Saturday and Sunday\n",
    "    running_saturday = running_saturday + datetime.timedelta(7)\n",
    "    running_sunday = running_sunday + datetime.timedelta(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roll the convolution over each traffic component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tommi/.local/lib/python3.6/site-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Container lists\n",
    "traffic_retail_smooth = []\n",
    "traffic_supermarket_smooth = []\n",
    "traffic_parks_smooth = []\n",
    "traffic_transit_stations_smooth = []\n",
    "traffic_workplaces_smooth = []\n",
    "traffic_residential_smooth = []\n",
    "\n",
    "# European regions\n",
    "european_regions_with_enough_data = list(dict.fromkeys(df_days_by_regions['region'].tolist()))\n",
    "\n",
    "# Loop over each European country\n",
    "for current_region in european_regions_with_enough_data:\n",
    "    \n",
    "    # Define the current country, a temporary dataframe of the country and x-axis (dates)\n",
    "    df_current = df_days_by_regions[(df_days_by_regions['region'] == current_region)]\n",
    "        \n",
    "    # Roll a window over the retail traffic to get it smoothened\n",
    "    df_current['traffic_retail_smooth']=df_current.traffic_retail.rolling(window = w2, win_type=None).mean()\n",
    "    df_current['traffic_retail_smooth']=df_current.traffic_retail_smooth.fillna(method='ffill')\n",
    "    df_current['traffic_retail_smooth']=df_current.traffic_retail_smooth.fillna(method='bfill').apply(np.float64)\n",
    "\n",
    "    # Roll a window over the supermarket traffic to get it smoothened\n",
    "    df_current['traffic_supermarket_smooth']=df_current.traffic_supermarket.rolling(window = w2, win_type=None).mean()\n",
    "    df_current['traffic_supermarket_smooth']=df_current.traffic_supermarket_smooth.fillna(method='ffill')\n",
    "    df_current['traffic_supermarket_smooth']=df_current.traffic_supermarket_smooth.fillna(method='bfill').apply(np.float64)\n",
    "    \n",
    "    # Roll a window over the park traffic to get it smoothened\n",
    "    df_current['traffic_parks_smooth']=df_current.traffic_parks.rolling(window = w2, win_type=None).mean()\n",
    "    df_current['traffic_parks_smooth']=df_current.traffic_parks_smooth.fillna(method='ffill')\n",
    "    df_current['traffic_parks_smooth']=df_current.traffic_parks_smooth.fillna(method='bfill').apply(np.float64)\n",
    "    \n",
    "    # Roll a window over the transit station traffic to get it smoothened\n",
    "    df_current['traffic_transit_stations_smooth']=df_current.traffic_transit_stations.rolling(window = w2, win_type=None).mean()\n",
    "    df_current['traffic_transit_stations_smooth']=df_current.traffic_transit_stations_smooth.fillna(method='ffill')\n",
    "    df_current['traffic_transit_stations_smooth']=df_current.traffic_transit_stations_smooth.fillna(method='bfill').apply(np.float64)\n",
    "    \n",
    "    # Roll a window over the workplace traffic to get it smoothened\n",
    "    # The weekend values are NaNs and therefore ignored\n",
    "    df_current['traffic_workplaces_smooth']=df_current.traffic_workplaces.rolling(window = w2, win_type=None, min_periods=1).mean()\n",
    "    df_current['traffic_workplaces_smooth']=df_current.traffic_workplaces_smooth.fillna(method='ffill')\n",
    "    df_current['traffic_workplaces_smooth']=df_current.traffic_workplaces_smooth.fillna(method='bfill').apply(np.float64)\n",
    "    \n",
    "    # Roll a window over the residential traffic to get it smoothened\n",
    "    df_current['traffic_residential_smooth']=df_current.traffic_residential.rolling(window = w2, win_type=None).mean()\n",
    "    df_current['traffic_residential_smooth']=df_current.traffic_residential_smooth.fillna(method='ffill')\n",
    "    df_current['traffic_residential_smooth']=df_current.traffic_residential_smooth.fillna(method='bfill').apply(np.float64)\n",
    "    \n",
    "    \n",
    "    # Add the current smoothened infected and death data to the container lists\n",
    "    traffic_retail_smooth = traffic_retail_smooth + df_current['traffic_retail_smooth'].tolist()\n",
    "    traffic_supermarket_smooth = traffic_supermarket_smooth + df_current['traffic_supermarket_smooth'].tolist()\n",
    "    traffic_parks_smooth = traffic_parks_smooth + df_current['traffic_parks_smooth'].tolist()\n",
    "    traffic_transit_stations_smooth = traffic_transit_stations_smooth + df_current['traffic_transit_stations_smooth'].tolist()\n",
    "    traffic_workplaces_smooth = traffic_workplaces_smooth + df_current['traffic_workplaces_smooth'].tolist()\n",
    "    traffic_residential_smooth = traffic_residential_smooth + df_current['traffic_residential_smooth'].tolist()\n",
    "    \n",
    "\n",
    "# Update the dataframe columns at once\n",
    "df_days_by_regions['traffic_retail_smooth'] = traffic_retail_smooth\n",
    "df_days_by_regions['traffic_supermarket_smooth'] = traffic_supermarket_smooth\n",
    "df_days_by_regions['traffic_parks_smooth'] = traffic_parks_smooth\n",
    "df_days_by_regions['traffic_transit_stations_smooth'] = traffic_transit_stations_smooth\n",
    "df_days_by_regions['traffic_workplaces_smooth'] = traffic_workplaces_smooth\n",
    "df_days_by_regions['traffic_residential_smooth'] = traffic_residential_smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Add the regional infected data to the dataframe\n",
    "\n",
    "- This section is a lot of monotone data reading. Used csv-files differ from each other differently which causes some repetitiveness in this section's code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this line, the provinces are automatically selected\n",
    "data_netherlands = data_netherlands[data_netherlands['Municipality_code'].isnull()]\n",
    "\n",
    "# Remove right away the columns which are not needed\n",
    "data_netherlands = data_netherlands.drop(columns=['Municipality_code', 'Municipality_name', 'Hospital_admission', 'Deceased', 'Error_value'])\n",
    "\n",
    "\n",
    "# Some regions in Dutch are different than the English versions that Google uses\n",
    "regions_in_dutch = ['Drenthe', 'Flevoland', 'Friesland', 'Gelderland', 'Groningen',\n",
    "                    'Limburg', 'Noord-Brabant', 'Noord-Holland', 'Overijssel', 'Zuid-Holland',\n",
    "                    'Utrecht', 'Zeeland']\n",
    "corresponding_regions = ['Drenthe', 'Flevoland', 'Friesland', 'Gelderland', 'Groningen',\n",
    "                         'Limburg', 'North Brabant', 'North Holland', 'Overijssel', 'South Holland',\n",
    "                         'Utrecht', 'Zeeland']\n",
    "\n",
    "\n",
    "# Get the first and the last date of the dataframe\n",
    "first_date_in_dataframe_netherlands = datetime.datetime.strptime(data_netherlands['Date_of_report'].tolist()[0], '%Y-%m-%d %H:%M:%S')\n",
    "last_date_in_dataframe_netherlands = min(observations_end_date, datetime.datetime.strptime(data_netherlands['Date_of_report'].tolist()[-1], '%Y-%m-%d %H:%M:%S')) \n",
    "\n",
    "\n",
    "\n",
    "# THIS ERROR MESSAGE CAN EVENTUALLY BE REMOVED\n",
    "# This cell prints a warning if observations_end_date was chosen to be too large\n",
    "if last_date_in_dataframe_netherlands > datetime.datetime.strptime(data_netherlands['Date_of_report'].tolist()[-1], '%Y-%m-%d %H:%M:%S'):\n",
    "    raise ValueError(\"Download Netherlands-dataset again\")\n",
    "\n",
    "\n",
    "# Region index\n",
    "for i in range(len(regions_in_dutch)):\n",
    "    \n",
    "    # Current region\n",
    "    current_region_in_dutch = regions_in_dutch[i]\n",
    "    # and normally\n",
    "    current_region = corresponding_regions[i]\n",
    "        \n",
    "    # New regional values\n",
    "    new_values = data_netherlands.loc[(data_netherlands['Province'] == current_region_in_dutch) & \n",
    "            (data_netherlands['Date_of_report'] >= first_date_in_dataframe_netherlands.strftime('%Y-%m-%d %H:%M:%S')) &\n",
    "            (data_netherlands['Date_of_report'] < last_date_in_dataframe_netherlands.strftime('%Y-%m-%d')), 'Total_reported'].tolist()\n",
    "    \n",
    "    # Add these values to the dataframe df_days_by_regions\n",
    "    df_days_by_regions.loc[(df_days_by_regions['region'] == current_region) & \n",
    "       (df_days_by_regions['date'] >= first_date_in_dataframe_netherlands.strftime('%Y-%m-%d')) &\n",
    "       (df_days_by_regions['date'] < last_date_in_dataframe_netherlands.strftime('%Y-%m-%d')), 'new_infections'] = new_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_2_letters = ['AN', 'AR', 'AS', 'CB', 'CE', \n",
    "                     'CL', 'CM', 'CN', 'CT', 'EX', \n",
    "                     'GA', 'IB', 'MC', 'MD', 'ML',\n",
    "                     'NC', 'PV', 'RI', 'VC']\n",
    "corresponding_regions = [ 'Andalusia', 'Aragon', 'Asturias', 'Cantabria', 'Ceuta',\n",
    "            'Castile and León', 'Castile-La Mancha', 'Canary Islands', 'Catalonia', 'Extremadura',\n",
    "            'Galicia', 'Balearic Islands', 'Region of Murcia', 'Community of Madrid', 'Melilla',\n",
    "            'Navarre', 'Basque Country', 'La Rioja', 'Valencian Community' ]\n",
    "\n",
    "\n",
    "# Get the first and the last date of the dataframe\n",
    "first_date_in_dataframe_spain = datetime.datetime.strptime(data_spain['fecha'].tolist()[0], '%Y-%m-%d')\n",
    "last_date_in_dataframe_spain = min(observations_end_date, datetime.datetime.strptime(data_spain['fecha'].tolist()[-1], '%Y-%m-%d'))\n",
    "\n",
    "\n",
    "\n",
    "# THIS ERROR MESSAGE CAN EVENTUALLY BE REMOVED\n",
    "# This cell prints a warning if observations_end_date was chosen to be too large\n",
    "if last_date_in_dataframe_spain > datetime.datetime.strptime(data_spain['fecha'].tolist()[-1], '%Y-%m-%d'):\n",
    "    raise ValueError(\"Download Spain-dataset again\")\n",
    "    \n",
    "    \n",
    "# Region index\n",
    "for i in range(len(regions_2_letters)):\n",
    "    \n",
    "    # Current region, written in 2 letters\n",
    "    current_region_2_letters = regions_2_letters[i]\n",
    "    # and normally\n",
    "    current_region = corresponding_regions[i]\n",
    "        \n",
    "    # New regional values\n",
    "    new_values = data_spain.loc[(data_spain['ccaa_iso'] == current_region_2_letters) & \n",
    "            (data_spain['fecha'] > first_date_in_dataframe_spain.strftime('%Y-%m-%d')) &\n",
    "            (data_spain['fecha'] < last_date_in_dataframe_spain.strftime('%Y-%m-%d')), 'num_casos'].tolist()\n",
    "        \n",
    "    # Add these values to the dataframe df_days_by_regions\n",
    "    df_days_by_regions.loc[(df_days_by_regions['region'] == current_region) & \n",
    "       (df_days_by_regions['date'] >= first_date_in_dataframe_spain.strftime('%Y-%m-%d')) &\n",
    "       (df_days_by_regions['date'] < last_date_in_dataframe_spain.strftime('%Y-%m-%d')), 'new_infections'] = new_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Create a csv-file out of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = os.path.join('Fully_implemented_dataframes', 'df_days_by_regions.csv')\n",
    "df_days_by_regions.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create df_regions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Add columns 'country' and 'region' to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# European regions\n",
    "european_regions_with_enough_data = list(dict.fromkeys(df_days_by_regions['region'].tolist()))\n",
    "\n",
    "# Get corresponding country for each region\n",
    "corresponding_countries = []\n",
    "for current_region in european_regions_with_enough_data:\n",
    "    corresponding_countries.append(\n",
    "    df_days_by_regions.loc[df_days_by_regions['region'] == current_region]['country'].iloc[0] \n",
    "    )\n",
    "    \n",
    "    \n",
    "# Update the dataframe columns at once\n",
    "df_regions['country'] = corresponding_countries\n",
    "df_regions['region'] = european_regions_with_enough_data\n",
    "\n",
    "# df_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 485 regions to be analysed even if there are only 29 European countries in this notebook!\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \" + str(df_regions.shape[0]) + \" regions to be analysed even if there are only \" +  str(num_countries) + \" European countries in this notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Create a csv-file out of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = os.path.join('Temporary_dataframes', 'df_regions.csv')\n",
    "df_regions.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
